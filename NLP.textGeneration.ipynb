{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this Jupyter I used LSTM to generate text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the file\n",
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        Doc= f.read()\n",
    "    return Doc    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1198623\n",
    "Doc=read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en',disable=['parser', 'tagger','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punch(Doc):\n",
    "    D=[]\n",
    "    for i in nlp(Doc):\n",
    "        if i.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ':\n",
    "             D.append(i.text.lower())\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file('melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=clean_punch(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_leg= 25+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want the model to do prediction based on 25 given worlds, So we make a list such that each element of the list contains \n",
    "# 25 token\n",
    "Sequence=[]\n",
    "for i in range(seq_leg, len(token)):\n",
    "    train_token= token[i-seq_leg:i]\n",
    "    Sequence.append(train_token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we encode the elements of the Sequence to integer using keras\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(Sequence)\n",
    "int_Sequence= tokenizer.texts_to_sequences(Sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "int_matrix=np.array(int_Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making input X and output y\n",
    "X=int_matrix[:,:-1]\n",
    "y=int_matrix[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning y to categorical \n",
    "from keras.utils import to_categorical\n",
    "vocab_size= len(tokenizer.word_counts)\n",
    "y= to_categorical(y, num_classes= vocab_size +1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 25)            438200    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 25, 150)           105600    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 17528)             2646728   \n",
      "=================================================================\n",
      "Total params: 3,393,778\n",
      "Trainable params: 3,393,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "model = create_model(vocab_size+1, seq_leg-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/300\n",
      "214686/214686 [==============================] - 319s 1ms/step - loss: 6.9717 - accuracy: 0.0683\n",
      "Epoch 2/300\n",
      "214686/214686 [==============================] - 281s 1ms/step - loss: 6.6157 - accuracy: 0.0817\n",
      "Epoch 3/300\n",
      "214686/214686 [==============================] - 280s 1ms/step - loss: 6.4503 - accuracy: 0.0862\n",
      "Epoch 4/300\n",
      "214686/214686 [==============================] - 269s 1ms/step - loss: 6.3003 - accuracy: 0.0915\n",
      "Epoch 5/300\n",
      "214686/214686 [==============================] - 283s 1ms/step - loss: 6.1635 - accuracy: 0.1006\n",
      "Epoch 6/300\n",
      "214686/214686 [==============================] - 273s 1ms/step - loss: 6.0454 - accuracy: 0.1053\n",
      "Epoch 7/300\n",
      "214686/214686 [==============================] - 273s 1ms/step - loss: 5.9528 - accuracy: 0.1088\n",
      "Epoch 8/300\n",
      "214686/214686 [==============================] - 274s 1ms/step - loss: 5.8305 - accuracy: 0.1122\n",
      "Epoch 9/300\n",
      "214686/214686 [==============================] - 353s 2ms/step - loss: 5.7178 - accuracy: 0.1146\n",
      "Epoch 10/300\n",
      "214686/214686 [==============================] - 278s 1ms/step - loss: 5.6142 - accuracy: 0.1173\n",
      "Epoch 11/300\n",
      "214686/214686 [==============================] - 276s 1ms/step - loss: 5.5174 - accuracy: 0.1189\n",
      "Epoch 12/300\n",
      "214686/214686 [==============================] - 275s 1ms/step - loss: 5.4219 - accuracy: 0.1212\n",
      "Epoch 13/300\n",
      "214686/214686 [==============================] - 276s 1ms/step - loss: 5.3291 - accuracy: 0.1236\n",
      "Epoch 14/300\n",
      "214686/214686 [==============================] - 287s 1ms/step - loss: 5.2375 - accuracy: 0.1259\n",
      "Epoch 15/300\n",
      "214686/214686 [==============================] - 297s 1ms/step - loss: 5.1492 - accuracy: 0.1279\n",
      "Epoch 16/300\n",
      "214686/214686 [==============================] - 294s 1ms/step - loss: 5.0624 - accuracy: 0.1304\n",
      "Epoch 17/300\n",
      "214686/214686 [==============================] - 295s 1ms/step - loss: 4.9780 - accuracy: 0.1329\n",
      "Epoch 18/300\n",
      "214686/214686 [==============================] - 301s 1ms/step - loss: 4.8977 - accuracy: 0.1365\n",
      "Epoch 19/300\n",
      "214686/214686 [==============================] - 299s 1ms/step - loss: 4.8204 - accuracy: 0.1403\n",
      "Epoch 20/300\n",
      "214686/214686 [==============================] - 303s 1ms/step - loss: 4.7459 - accuracy: 0.1442\n",
      "Epoch 21/300\n",
      "214686/214686 [==============================] - 303s 1ms/step - loss: 4.6775 - accuracy: 0.1486\n",
      "Epoch 22/300\n",
      "214686/214686 [==============================] - 300s 1ms/step - loss: 4.6110 - accuracy: 0.1538\n",
      "Epoch 23/300\n",
      "214686/214686 [==============================] - 301s 1ms/step - loss: 4.5498 - accuracy: 0.1589\n",
      "Epoch 24/300\n",
      "214686/214686 [==============================] - 293s 1ms/step - loss: 4.4904 - accuracy: 0.1640\n",
      "Epoch 25/300\n",
      "214686/214686 [==============================] - 292s 1ms/step - loss: 4.4349 - accuracy: 0.1687\n",
      "Epoch 26/300\n",
      "214686/214686 [==============================] - 296s 1ms/step - loss: 4.3830 - accuracy: 0.1733\n",
      "Epoch 27/300\n",
      "214686/214686 [==============================] - 281s 1ms/step - loss: 4.3321 - accuracy: 0.1788\n",
      "Epoch 28/300\n",
      "214686/214686 [==============================] - 305s 1ms/step - loss: 4.2836 - accuracy: 0.1842\n",
      "Epoch 29/300\n",
      "214686/214686 [==============================] - 316s 1ms/step - loss: 4.2349 - accuracy: 0.1884\n",
      "Epoch 30/300\n",
      "214686/214686 [==============================] - 305s 1ms/step - loss: 4.1898 - accuracy: 0.1930\n",
      "Epoch 31/300\n",
      "214686/214686 [==============================] - 308s 1ms/step - loss: 4.1440 - accuracy: 0.1985\n",
      "Epoch 32/300\n",
      "214686/214686 [==============================] - 299s 1ms/step - loss: 4.1021 - accuracy: 0.2030\n",
      "Epoch 33/300\n",
      "214686/214686 [==============================] - 289s 1ms/step - loss: 4.0574 - accuracy: 0.2079\n",
      "Epoch 34/300\n",
      "214686/214686 [==============================] - 311s 1ms/step - loss: 4.0152 - accuracy: 0.2135\n",
      "Epoch 35/300\n",
      "214686/214686 [==============================] - 285s 1ms/step - loss: 3.9749 - accuracy: 0.2187\n",
      "Epoch 36/300\n",
      "214686/214686 [==============================] - 279s 1ms/step - loss: 3.9339 - accuracy: 0.2233\n",
      "Epoch 37/300\n",
      "214686/214686 [==============================] - 275s 1ms/step - loss: 3.8946 - accuracy: 0.2276\n",
      "Epoch 38/300\n",
      "214686/214686 [==============================] - 274s 1ms/step - loss: 3.8542 - accuracy: 0.2324\n",
      "Epoch 39/300\n",
      "214686/214686 [==============================] - 288s 1ms/step - loss: 3.8159 - accuracy: 0.2374\n",
      "Epoch 40/300\n",
      "214686/214686 [==============================] - 282s 1ms/step - loss: 3.7773 - accuracy: 0.2430\n",
      "Epoch 41/300\n",
      "214686/214686 [==============================] - 278s 1ms/step - loss: 3.7398 - accuracy: 0.2480\n",
      "Epoch 42/300\n",
      "214686/214686 [==============================] - 282s 1ms/step - loss: 3.7010 - accuracy: 0.2526\n",
      "Epoch 43/300\n",
      "214686/214686 [==============================] - 286s 1ms/step - loss: 3.6657 - accuracy: 0.2573\n",
      "Epoch 44/300\n",
      "214686/214686 [==============================] - 289s 1ms/step - loss: 3.6292 - accuracy: 0.2618\n",
      "Epoch 45/300\n",
      "214686/214686 [==============================] - 288s 1ms/step - loss: 3.5949 - accuracy: 0.2669\n",
      "Epoch 46/300\n",
      "214686/214686 [==============================] - 310s 1ms/step - loss: 3.5607 - accuracy: 0.2714\n",
      "Epoch 47/300\n",
      "214686/214686 [==============================] - 291s 1ms/step - loss: 3.5251 - accuracy: 0.2760\n",
      "Epoch 48/300\n",
      "214686/214686 [==============================] - 282s 1ms/step - loss: 3.4921 - accuracy: 0.2802\n",
      "Epoch 49/300\n",
      "214686/214686 [==============================] - 283s 1ms/step - loss: 3.4561 - accuracy: 0.2857\n",
      "Epoch 50/300\n",
      "214686/214686 [==============================] - 288s 1ms/step - loss: 3.4255 - accuracy: 0.2904\n",
      "Epoch 51/300\n",
      "214686/214686 [==============================] - 1363s 6ms/step - loss: 3.3920 - accuracy: 0.2958\n",
      "Epoch 52/300\n",
      "214686/214686 [==============================] - 277s 1ms/step - loss: 3.3606 - accuracy: 0.2995\n",
      "Epoch 53/300\n",
      "214686/214686 [==============================] - 293s 1ms/step - loss: 3.3294 - accuracy: 0.3033\n",
      "Epoch 54/300\n",
      "214686/214686 [==============================] - 301s 1ms/step - loss: 3.2956 - accuracy: 0.3083\n",
      "Epoch 55/300\n",
      "214686/214686 [==============================] - 298s 1ms/step - loss: 3.2666 - accuracy: 0.3130\n",
      "Epoch 56/300\n",
      "214686/214686 [==============================] - 281s 1ms/step - loss: 3.2360 - accuracy: 0.3182\n",
      "Epoch 57/300\n",
      "214686/214686 [==============================] - 309s 1ms/step - loss: 3.2058 - accuracy: 0.3211\n",
      "Epoch 58/300\n",
      "214686/214686 [==============================] - 332s 2ms/step - loss: 3.1783 - accuracy: 0.3268\n",
      "Epoch 59/300\n",
      "214686/214686 [==============================] - 340s 2ms/step - loss: 3.1433 - accuracy: 0.3314\n",
      "Epoch 60/300\n",
      "214686/214686 [==============================] - 318s 1ms/step - loss: 3.1181 - accuracy: 0.3351\n",
      "Epoch 61/300\n",
      "214686/214686 [==============================] - 315s 1ms/step - loss: 3.0899 - accuracy: 0.3401\n",
      "Epoch 62/300\n",
      "214686/214686 [==============================] - 316s 1ms/step - loss: 3.0617 - accuracy: 0.3444\n",
      "Epoch 63/300\n",
      "214686/214686 [==============================] - 290s 1ms/step - loss: 3.0362 - accuracy: 0.3485\n",
      "Epoch 64/300\n",
      "214686/214686 [==============================] - 297s 1ms/step - loss: 3.0096 - accuracy: 0.3516\n",
      "Epoch 65/300\n",
      "214686/214686 [==============================] - 291s 1ms/step - loss: 2.9819 - accuracy: 0.3561\n",
      "Epoch 66/300\n",
      "214686/214686 [==============================] - 290s 1ms/step - loss: 2.9550 - accuracy: 0.3607\n",
      "Epoch 67/300\n",
      "214686/214686 [==============================] - 287s 1ms/step - loss: 2.9262 - accuracy: 0.3650\n",
      "Epoch 68/300\n",
      "214686/214686 [==============================] - 291s 1ms/step - loss: 2.9020 - accuracy: 0.3687\n",
      "Epoch 69/300\n",
      "214686/214686 [==============================] - 292s 1ms/step - loss: 2.8774 - accuracy: 0.3729\n",
      "Epoch 70/300\n",
      "214686/214686 [==============================] - 293s 1ms/step - loss: 2.8522 - accuracy: 0.3770\n",
      "Epoch 71/300\n",
      "214686/214686 [==============================] - 309s 1ms/step - loss: 2.8263 - accuracy: 0.3808\n",
      "Epoch 72/300\n",
      "214686/214686 [==============================] - 308s 1ms/step - loss: 2.8024 - accuracy: 0.3853\n",
      "Epoch 73/300\n",
      "214686/214686 [==============================] - 292s 1ms/step - loss: 2.7825 - accuracy: 0.3880\n",
      "Epoch 74/300\n",
      "214686/214686 [==============================] - 324s 2ms/step - loss: 2.7566 - accuracy: 0.3926\n",
      "Epoch 75/300\n",
      "214686/214686 [==============================] - 309s 1ms/step - loss: 2.7342 - accuracy: 0.3959\n",
      "Epoch 76/300\n",
      "214686/214686 [==============================] - 331s 2ms/step - loss: 2.7091 - accuracy: 0.4001\n",
      "Epoch 77/300\n",
      "214686/214686 [==============================] - 299s 1ms/step - loss: 2.6867 - accuracy: 0.4052\n",
      "Epoch 78/300\n",
      "214686/214686 [==============================] - 280s 1ms/step - loss: 2.6672 - accuracy: 0.4085\n",
      "Epoch 79/300\n",
      "214686/214686 [==============================] - 283s 1ms/step - loss: 2.6428 - accuracy: 0.4120\n",
      "Epoch 80/300\n",
      "214686/214686 [==============================] - 285s 1ms/step - loss: 2.6243 - accuracy: 0.4147\n",
      "Epoch 81/300\n",
      "214686/214686 [==============================] - 281s 1ms/step - loss: 2.6010 - accuracy: 0.4185\n",
      "Epoch 82/300\n",
      "214686/214686 [==============================] - 287s 1ms/step - loss: 2.5805 - accuracy: 0.4221\n",
      "Epoch 83/300\n",
      "214686/214686 [==============================] - 282s 1ms/step - loss: 2.5651 - accuracy: 0.4251\n",
      "Epoch 84/300\n",
      "214686/214686 [==============================] - 299s 1ms/step - loss: 2.5403 - accuracy: 0.4301\n",
      "Epoch 85/300\n",
      "214686/214686 [==============================] - 288s 1ms/step - loss: 2.5208 - accuracy: 0.4327\n",
      "Epoch 86/300\n",
      "214686/214686 [==============================] - 301s 1ms/step - loss: 2.4983 - accuracy: 0.4367\n",
      "Epoch 87/300\n",
      "214686/214686 [==============================] - 291s 1ms/step - loss: 2.4830 - accuracy: 0.4389\n",
      "Epoch 88/300\n",
      "214686/214686 [==============================] - 291s 1ms/step - loss: 2.4656 - accuracy: 0.4426\n",
      "Epoch 89/300\n",
      "214686/214686 [==============================] - 287s 1ms/step - loss: 2.4433 - accuracy: 0.4464\n",
      "Epoch 90/300\n",
      "214686/214686 [==============================] - 281s 1ms/step - loss: 2.4281 - accuracy: 0.4491\n",
      "Epoch 91/300\n",
      "214686/214686 [==============================] - 282s 1ms/step - loss: 2.4119 - accuracy: 0.4524\n",
      "Epoch 92/300\n",
      "214686/214686 [==============================] - 300s 1ms/step - loss: 2.3947 - accuracy: 0.4547\n",
      "Epoch 93/300\n",
      "214686/214686 [==============================] - 286s 1ms/step - loss: 2.3734 - accuracy: 0.4585\n",
      "Epoch 94/300\n",
      "214686/214686 [==============================] - 270s 1ms/step - loss: 2.3573 - accuracy: 0.4620\n",
      "Epoch 95/300\n",
      "214686/214686 [==============================] - 280s 1ms/step - loss: 2.3423 - accuracy: 0.4642\n",
      "Epoch 96/300\n",
      "214686/214686 [==============================] - 276s 1ms/step - loss: 2.3270 - accuracy: 0.4663\n",
      "Epoch 97/300\n",
      "214686/214686 [==============================] - 273s 1ms/step - loss: 2.3086 - accuracy: 0.4708\n",
      "Epoch 98/300\n",
      "214686/214686 [==============================] - 274s 1ms/step - loss: 2.2915 - accuracy: 0.4730\n",
      "Epoch 99/300\n",
      "214686/214686 [==============================] - 273s 1ms/step - loss: 2.2776 - accuracy: 0.4752\n",
      "Epoch 100/300\n",
      "214686/214686 [==============================] - 274s 1ms/step - loss: 2.2596 - accuracy: 0.4794\n",
      "Epoch 101/300\n",
      "214686/214686 [==============================] - 269s 1ms/step - loss: 2.2460 - accuracy: 0.4818\n",
      "Epoch 102/300\n",
      "214686/214686 [==============================] - 270s 1ms/step - loss: 2.2325 - accuracy: 0.4836\n",
      "Epoch 103/300\n",
      "214686/214686 [==============================] - 269s 1ms/step - loss: 2.2210 - accuracy: 0.4860\n",
      "Epoch 104/300\n",
      "214686/214686 [==============================] - 268s 1ms/step - loss: 2.2048 - accuracy: 0.4894\n",
      "Epoch 105/300\n",
      "214686/214686 [==============================] - 270s 1ms/step - loss: 2.1873 - accuracy: 0.4915\n",
      "Epoch 106/300\n",
      "214686/214686 [==============================] - 271s 1ms/step - loss: 2.1782 - accuracy: 0.4941\n",
      "Epoch 107/300\n",
      "214686/214686 [==============================] - 270s 1ms/step - loss: 2.1679 - accuracy: 0.4950\n",
      "Epoch 108/300\n",
      "214686/214686 [==============================] - 270s 1ms/step - loss: 2.1562 - accuracy: 0.4988\n",
      "Epoch 109/300\n",
      "214686/214686 [==============================] - 269s 1ms/step - loss: 2.1380 - accuracy: 0.5012\n",
      "Epoch 110/300\n",
      "214686/214686 [==============================] - 268s 1ms/step - loss: 2.1272 - accuracy: 0.5027\n",
      "Epoch 111/300\n",
      "214686/214686 [==============================] - 267s 1ms/step - loss: 2.1120 - accuracy: 0.5064\n",
      "Epoch 112/300\n",
      "214686/214686 [==============================] - 267s 1ms/step - loss: 2.1019 - accuracy: 0.5075\n",
      "Epoch 113/300\n",
      "214686/214686 [==============================] - 268s 1ms/step - loss: 2.0930 - accuracy: 0.5093\n",
      "Epoch 114/300\n",
      "214686/214686 [==============================] - 266s 1ms/step - loss: 2.0792 - accuracy: 0.5115\n",
      "Epoch 115/300\n",
      "214686/214686 [==============================] - 266s 1ms/step - loss: 2.0646 - accuracy: 0.5146\n",
      "Epoch 116/300\n",
      "214686/214686 [==============================] - 266s 1ms/step - loss: 2.0578 - accuracy: 0.5162\n",
      "Epoch 117/300\n",
      "214686/214686 [==============================] - 266s 1ms/step - loss: 2.0489 - accuracy: 0.5180\n",
      "Epoch 118/300\n",
      "214686/214686 [==============================] - 266s 1ms/step - loss: 2.0337 - accuracy: 0.5212\n",
      "Epoch 119/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 2.0219 - accuracy: 0.5229\n",
      "Epoch 120/300\n",
      "214686/214686 [==============================] - 266s 1ms/step - loss: 2.0133 - accuracy: 0.5252\n",
      "Epoch 121/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 2.0003 - accuracy: 0.5267\n",
      "Epoch 122/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 1.9846 - accuracy: 0.5297\n",
      "Epoch 123/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 1.9796 - accuracy: 0.5307\n",
      "Epoch 124/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 1.9642 - accuracy: 0.5332\n",
      "Epoch 125/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 1.9617 - accuracy: 0.5331\n",
      "Epoch 126/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 1.9570 - accuracy: 0.5335\n",
      "Epoch 127/300\n",
      "214686/214686 [==============================] - 266s 1ms/step - loss: 1.9391 - accuracy: 0.5378\n",
      "Epoch 128/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 1.9299 - accuracy: 0.5401\n",
      "Epoch 129/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.9178 - accuracy: 0.5426\n",
      "Epoch 130/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 1.9126 - accuracy: 0.5422\n",
      "Epoch 131/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.9074 - accuracy: 0.5440\n",
      "Epoch 132/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.8936 - accuracy: 0.5483\n",
      "Epoch 133/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.8801 - accuracy: 0.5489\n",
      "Epoch 134/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.8774 - accuracy: 0.5496\n",
      "Epoch 135/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 1.8723 - accuracy: 0.5511\n",
      "Epoch 136/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.8599 - accuracy: 0.5539\n",
      "Epoch 137/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.8519 - accuracy: 0.5548\n",
      "Epoch 138/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 1.8408 - accuracy: 0.5574\n",
      "Epoch 139/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.8341 - accuracy: 0.5572\n",
      "Epoch 140/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.8245 - accuracy: 0.5610\n",
      "Epoch 141/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.8254 - accuracy: 0.5601\n",
      "Epoch 142/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.8111 - accuracy: 0.5626\n",
      "Epoch 143/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.8051 - accuracy: 0.5640\n",
      "Epoch 144/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.7918 - accuracy: 0.5669\n",
      "Epoch 145/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.7899 - accuracy: 0.5672\n",
      "Epoch 146/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.7783 - accuracy: 0.5698\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.7777 - accuracy: 0.5701\n",
      "Epoch 148/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.7686 - accuracy: 0.5712\n",
      "Epoch 149/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.7581 - accuracy: 0.5740\n",
      "Epoch 150/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.7514 - accuracy: 0.5741\n",
      "Epoch 151/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.7425 - accuracy: 0.5764\n",
      "Epoch 152/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.7372 - accuracy: 0.5790\n",
      "Epoch 153/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.7389 - accuracy: 0.5775\n",
      "Epoch 154/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.7192 - accuracy: 0.5813\n",
      "Epoch 155/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.7240 - accuracy: 0.5801\n",
      "Epoch 156/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.7144 - accuracy: 0.5833\n",
      "Epoch 157/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.6993 - accuracy: 0.5851\n",
      "Epoch 158/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.7037 - accuracy: 0.5850\n",
      "Epoch 159/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.6988 - accuracy: 0.5858\n",
      "Epoch 160/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.6823 - accuracy: 0.5894\n",
      "Epoch 161/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.6835 - accuracy: 0.5886\n",
      "Epoch 162/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.6764 - accuracy: 0.5902\n",
      "Epoch 163/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.6724 - accuracy: 0.5908\n",
      "Epoch 164/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.6549 - accuracy: 0.5952\n",
      "Epoch 165/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.6594 - accuracy: 0.5936\n",
      "Epoch 166/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.6574 - accuracy: 0.5931\n",
      "Epoch 167/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.6556 - accuracy: 0.5934\n",
      "Epoch 168/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.6378 - accuracy: 0.5982\n",
      "Epoch 169/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.6371 - accuracy: 0.5983\n",
      "Epoch 170/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.6331 - accuracy: 0.5982\n",
      "Epoch 171/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.6331 - accuracy: 0.5992\n",
      "Epoch 172/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.6254 - accuracy: 0.6007\n",
      "Epoch 173/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.6158 - accuracy: 0.6028\n",
      "Epoch 174/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.6100 - accuracy: 0.6038\n",
      "Epoch 175/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.6011 - accuracy: 0.6046\n",
      "Epoch 176/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.6017 - accuracy: 0.6052\n",
      "Epoch 177/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5989 - accuracy: 0.6064\n",
      "Epoch 178/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5966 - accuracy: 0.6059\n",
      "Epoch 179/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5867 - accuracy: 0.6085\n",
      "Epoch 180/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5774 - accuracy: 0.6105\n",
      "Epoch 181/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.5821 - accuracy: 0.6091\n",
      "Epoch 182/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5770 - accuracy: 0.6096\n",
      "Epoch 183/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5654 - accuracy: 0.6121\n",
      "Epoch 184/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5615 - accuracy: 0.6138\n",
      "Epoch 185/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5686 - accuracy: 0.6124\n",
      "Epoch 186/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5497 - accuracy: 0.6168\n",
      "Epoch 187/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5477 - accuracy: 0.6162\n",
      "Epoch 188/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5572 - accuracy: 0.6148\n",
      "Epoch 189/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5425 - accuracy: 0.6175\n",
      "Epoch 190/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5451 - accuracy: 0.6165\n",
      "Epoch 191/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5262 - accuracy: 0.6208\n",
      "Epoch 192/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5237 - accuracy: 0.6202\n",
      "Epoch 193/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5351 - accuracy: 0.6187\n",
      "Epoch 194/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5240 - accuracy: 0.6208\n",
      "Epoch 195/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5154 - accuracy: 0.6238\n",
      "Epoch 196/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5115 - accuracy: 0.6236\n",
      "Epoch 197/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5052 - accuracy: 0.6244\n",
      "Epoch 198/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5112 - accuracy: 0.6241\n",
      "Epoch 199/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4975 - accuracy: 0.6261\n",
      "Epoch 200/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4979 - accuracy: 0.6268\n",
      "Epoch 201/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5002 - accuracy: 0.6251\n",
      "Epoch 202/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.4876 - accuracy: 0.6296\n",
      "Epoch 203/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4839 - accuracy: 0.6302\n",
      "Epoch 204/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.5120 - accuracy: 0.6231\n",
      "Epoch 205/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4740 - accuracy: 0.6317\n",
      "Epoch 206/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4712 - accuracy: 0.6319\n",
      "Epoch 207/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4798 - accuracy: 0.6302\n",
      "Epoch 208/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4769 - accuracy: 0.6304\n",
      "Epoch 209/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4584 - accuracy: 0.6350\n",
      "Epoch 210/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4649 - accuracy: 0.6330\n",
      "Epoch 211/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4544 - accuracy: 0.6358\n",
      "Epoch 212/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4653 - accuracy: 0.6338\n",
      "Epoch 213/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4547 - accuracy: 0.6356\n",
      "Epoch 214/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4561 - accuracy: 0.6350\n",
      "Epoch 215/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4436 - accuracy: 0.6383\n",
      "Epoch 216/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4459 - accuracy: 0.6361\n",
      "Epoch 217/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4460 - accuracy: 0.6376\n",
      "Epoch 218/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4349 - accuracy: 0.6396\n",
      "Epoch 219/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4293 - accuracy: 0.6411\n",
      "Epoch 220/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4283 - accuracy: 0.6416\n",
      "Epoch 221/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4213 - accuracy: 0.6429\n",
      "Epoch 222/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.4149 - accuracy: 0.6442\n",
      "Epoch 223/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4266 - accuracy: 0.6414\n",
      "Epoch 224/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4174 - accuracy: 0.6446\n",
      "Epoch 225/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4057 - accuracy: 0.6470\n",
      "Epoch 226/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4152 - accuracy: 0.6443\n",
      "Epoch 227/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4125 - accuracy: 0.6444\n",
      "Epoch 228/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4040 - accuracy: 0.6460\n",
      "Epoch 229/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.3927 - accuracy: 0.6490\n",
      "Epoch 230/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.4095 - accuracy: 0.6453\n",
      "Epoch 231/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3903 - accuracy: 0.6495\n",
      "Epoch 232/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3925 - accuracy: 0.6484\n",
      "Epoch 233/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3954 - accuracy: 0.6478\n",
      "Epoch 234/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 1.3958 - accuracy: 0.6482\n",
      "Epoch 235/300\n",
      "214686/214686 [==============================] - 270s 1ms/step - loss: 1.3855 - accuracy: 0.6503\n",
      "Epoch 236/300\n",
      "214686/214686 [==============================] - 266s 1ms/step - loss: 1.3842 - accuracy: 0.6508\n",
      "Epoch 237/300\n",
      "214686/214686 [==============================] - 274s 1ms/step - loss: 1.3652 - accuracy: 0.6550\n",
      "Epoch 238/300\n",
      "214686/214686 [==============================] - 282s 1ms/step - loss: 1.3791 - accuracy: 0.6520\n",
      "Epoch 239/300\n",
      "214686/214686 [==============================] - 272s 1ms/step - loss: 1.3785 - accuracy: 0.6520\n",
      "Epoch 240/300\n",
      "214686/214686 [==============================] - 268s 1ms/step - loss: 1.3829 - accuracy: 0.6502\n",
      "Epoch 241/300\n",
      "214686/214686 [==============================] - 269s 1ms/step - loss: 1.3680 - accuracy: 0.6540\n",
      "Epoch 242/300\n",
      "214686/214686 [==============================] - 268s 1ms/step - loss: 1.3633 - accuracy: 0.6548\n",
      "Epoch 243/300\n",
      "214686/214686 [==============================] - 268s 1ms/step - loss: 1.3581 - accuracy: 0.6564\n",
      "Epoch 244/300\n",
      "214686/214686 [==============================] - 268s 1ms/step - loss: 1.3664 - accuracy: 0.6537\n",
      "Epoch 245/300\n",
      "214686/214686 [==============================] - 267s 1ms/step - loss: 1.3637 - accuracy: 0.6544\n",
      "Epoch 246/300\n",
      "214686/214686 [==============================] - 269s 1ms/step - loss: 1.3558 - accuracy: 0.6567\n",
      "Epoch 247/300\n",
      "214686/214686 [==============================] - 267s 1ms/step - loss: 1.3549 - accuracy: 0.6568\n",
      "Epoch 248/300\n",
      "214686/214686 [==============================] - 268s 1ms/step - loss: 1.3404 - accuracy: 0.6601\n",
      "Epoch 249/300\n",
      "214686/214686 [==============================] - 267s 1ms/step - loss: 1.3419 - accuracy: 0.6595\n",
      "Epoch 250/300\n",
      "214686/214686 [==============================] - 266s 1ms/step - loss: 1.3504 - accuracy: 0.6574\n",
      "Epoch 251/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 1.3386 - accuracy: 0.6606\n",
      "Epoch 252/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.3453 - accuracy: 0.6581\n",
      "Epoch 253/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.3356 - accuracy: 0.6610\n",
      "Epoch 254/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.3295 - accuracy: 0.6622\n",
      "Epoch 255/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.3353 - accuracy: 0.6595\n",
      "Epoch 256/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.3186 - accuracy: 0.6652\n",
      "Epoch 257/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3229 - accuracy: 0.6640\n",
      "Epoch 258/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3262 - accuracy: 0.6634\n",
      "Epoch 259/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3341 - accuracy: 0.6617\n",
      "Epoch 260/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3196 - accuracy: 0.6643\n",
      "Epoch 261/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3118 - accuracy: 0.6666\n",
      "Epoch 262/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3143 - accuracy: 0.6660\n",
      "Epoch 263/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3225 - accuracy: 0.6631\n",
      "Epoch 264/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3231 - accuracy: 0.6630\n",
      "Epoch 265/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3037 - accuracy: 0.6670\n",
      "Epoch 266/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3069 - accuracy: 0.6675\n",
      "Epoch 267/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.3107 - accuracy: 0.6666\n",
      "Epoch 268/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3002 - accuracy: 0.6693\n",
      "Epoch 269/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3061 - accuracy: 0.6665\n",
      "Epoch 270/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2877 - accuracy: 0.6708\n",
      "Epoch 271/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2888 - accuracy: 0.6707\n",
      "Epoch 272/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2912 - accuracy: 0.6707\n",
      "Epoch 273/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.3000 - accuracy: 0.6686\n",
      "Epoch 274/300\n",
      "214686/214686 [==============================] - 264s 1ms/step - loss: 1.2858 - accuracy: 0.6722\n",
      "Epoch 275/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.2745 - accuracy: 0.6738\n",
      "Epoch 276/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2943 - accuracy: 0.6699\n",
      "Epoch 277/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2799 - accuracy: 0.6735\n",
      "Epoch 278/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2803 - accuracy: 0.6724\n",
      "Epoch 279/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2728 - accuracy: 0.6743\n",
      "Epoch 280/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2729 - accuracy: 0.6754\n",
      "Epoch 281/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2689 - accuracy: 0.6756\n",
      "Epoch 282/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2729 - accuracy: 0.6742\n",
      "Epoch 283/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2604 - accuracy: 0.6777\n",
      "Epoch 284/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2725 - accuracy: 0.6751\n",
      "Epoch 285/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2752 - accuracy: 0.6747\n",
      "Epoch 286/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2578 - accuracy: 0.6769\n",
      "Epoch 287/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2610 - accuracy: 0.6779\n",
      "Epoch 288/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2618 - accuracy: 0.6768\n",
      "Epoch 289/300\n",
      "214686/214686 [==============================] - 265s 1ms/step - loss: 1.2578 - accuracy: 0.6786\n",
      "Epoch 290/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2503 - accuracy: 0.6799\n",
      "Epoch 291/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2590 - accuracy: 0.6772\n",
      "Epoch 292/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2508 - accuracy: 0.6790\n",
      "Epoch 293/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.2436 - accuracy: 0.6806\n",
      "Epoch 294/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2503 - accuracy: 0.6802\n",
      "Epoch 295/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.2519 - accuracy: 0.6794\n",
      "Epoch 296/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.2407 - accuracy: 0.6817\n",
      "Epoch 297/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2312 - accuracy: 0.6834\n",
      "Epoch 298/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.2415 - accuracy: 0.6821\n",
      "Epoch 299/300\n",
      "214686/214686 [==============================] - 263s 1ms/step - loss: 1.2355 - accuracy: 0.6824\n",
      "Epoch 300/300\n",
      "214686/214686 [==============================] - 262s 1ms/step - loss: 1.2224 - accuracy: 0.6863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x9d215cb50>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=300,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('epochBIG.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('epochBIG', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    model : model that was trained on text data\n",
    "    tokenizer : tokenizer that was fit on text data\n",
    "    seq_len : length of training sequence\n",
    "    seed_text : raw string text to serve as the seed\n",
    "    num_gen_words : number of words to be generated by model\n",
    "    '''\n",
    "    \n",
    "    # Final Output\n",
    "    output_text = []\n",
    "    \n",
    "    # Intial Seed Sequence\n",
    "    input_text = seed_text\n",
    "    \n",
    "    # Create num_gen_words\n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "\n",
    "        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n",
    "        \n",
    "\n",
    "        pred_word = tokenizer.index_word[pred_word_ind] \n",
    "\n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0,len(Sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed_text = Sequence[random_pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ' '.join(random_seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stranger that stubb vowed he recognised his cutting spade pole entangled in the lines that were knotted round the tail of one of these whales there'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'s the places that in the fishery the captain sperm before quivering the oil was at all prudent blackness of gore and as a real crew and may seem to see either lifted of it and smelt sideways for succession--\"do they see that will i think over according to him'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = X.shape[1]\n",
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "were stains of some sort or other. At first I knew not what to make of this; but soon an inkling of the truth occurred to me. I remembered a story of a white man--a whaleman too--who, falling among the\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,word in enumerate(full_text.split()):\n",
    "    if word == 'inkling':\n",
    "        print(' '.join(full_text.split()[i-20:i+20]))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
